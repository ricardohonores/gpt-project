# GPT Project: RAG & Fine-tuning

[EspaÃ±ol](README.es.md) | English

Experimentation project with advanced AI techniques: **RAG (Retrieval-Augmented Generation)** and **Fine-tuning** using local language models.

## ğŸ¯ Description

This project implements two complementary approaches to enhance language model capabilities:

1. **RAG System**: Allows a local LLM to answer questions based on your own documents using vector search
2. **Fine-tuning with QLoRA**: Adapts a pre-trained model to specific tasks using 4-bit quantization

## âœ¨ Features

### RAG System (`chatrag_py.py`)
- ğŸ“š Processes PDF and TXT documents
- ğŸ” Semantic search with FAISS
- ğŸ¤– Integration with Ollama for local inference
- ğŸ’¬ Interactive Q&A interface
- ğŸ“ Shows information sources in each response

### Fine-tuning (`finetuning.py`)
- âš¡ QLoRA (Quantized Low-Rank Adaptation) for training with low VRAM
- ğŸ¯ 4-bit quantization
- ğŸ’¾ Saves only adapters (lightweight weights)
- ğŸ”§ Configurable for GPU or CPU

## ğŸ“‹ Requirements

- Python 3.8+
- [Ollama](https://ollama.ai/) installed and running
- `gpt-oss:20b` model downloaded in Ollama
- GPU with CUDA (optional, but recommended for fine-tuning)

## ğŸš€ Installation

### 1. Clone the repository
```bash
git clone https://github.com/ricardohonores/gpt-project.git
cd gpt-project
```

### 2. Create virtual environment
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### 3. Install dependencies

**For RAG system:**
```bash
pip install langchain-community langchain-core transformers accelerate sentence-transformers faiss-cpu pypdf
```

**For fine-tuning (additional):**
```bash
pip install datasets peft trl bitsandbytes
```

**For GPU (optional):**
```bash
pip install faiss-gpu
```

### 4. Install and configure Ollama
```bash
# Install Ollama (if you don't have it)
curl -fsSL https://ollama.ai/install.sh | sh

# Download the model
ollama pull gpt-oss:20b
```

## ğŸ’» Usage

### RAG System

1. **Place your documents** in the `mis_documentos/` folder (.txt or .pdf files)

2. **Start Ollama** (in another terminal):
```bash
ollama serve
```

3. **Run the RAG system**:
```bash
python chatrag_py.py
```

4. **Ask questions** interactively. Type `salir` to exit.

**Example:**
```
Your question: What is generative AI?

--- Answer ---
Generative AI is artificial intelligence that creates new content...

--- Sources ---
-> Source: /path/to/document.pdf, Page: 5
```

### Fine-tuning

1. **Prepare your dataset** in JSONL format (`dataset.jsonl`):
```json
{"instruction": "Question or task", "output": "Expected response"}
{"instruction": "Another question", "output": "Another response"}
```

2. **Run training**:
```bash
python finetuning.py
```

3. **Adapters are saved** in `./gpt_oss_fine_tuned/`

## ğŸ“ Project Structure

```
gpt-project/
â”œâ”€â”€ chatrag_py.py          # Complete RAG system
â”œâ”€â”€ finetuning.py          # Fine-tuning script with QLoRA
â”œâ”€â”€ dataset.jsonl          # Sample dataset
â”œâ”€â”€ CLAUDE.md              # Technical documentation (English)
â”œâ”€â”€ CLAUDE.es.md           # Technical documentation (Spanish)
â”œâ”€â”€ README.md              # This file (English)
â”œâ”€â”€ README.es.md           # README in Spanish
â”œâ”€â”€ .gitignore             # Files ignored by git
â””â”€â”€ mis_documentos/        # Document folder for RAG
    â”œâ”€â”€ *.pdf
    â””â”€â”€ *.txt
```

## ğŸ”§ Configuration

### RAG System
Edit the following variables in `chatrag_py.py`:
```python
DOCS_FOLDER = "/path/to/your/documents"
EMBEDDINGS_MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
OLLAMA_MODEL_NAME = "gpt-oss:20b"
```

### Fine-tuning
Edit the following variables in `finetuning.py`:
```python
OLLAMA_MODEL_NAME = "gpt-oss:20b"
DATASET_FILE = "dataset.jsonl"
OUTPUT_DIR = "./gpt_oss_fine_tuned"
```

## ğŸ¤” RAG or Fine-tuning?

| Feature | RAG | Fine-tuning |
|---------|-----|-------------|
| **Modifies the model** | âŒ No | âœ… Yes |
| **Requires training** | âŒ No | âœ… Yes |
| **Dynamic documents** | âœ… Yes | âŒ No |
| **Shows sources** | âœ… Yes | âŒ No |
| **Changes behavior** | âŒ No | âœ… Yes |
| **Memory usage** | ğŸŸ¢ Low | ğŸŸ¡ Medium |

**Use RAG when:** You need to answer questions about specific documents or knowledge that changes frequently.

**Use Fine-tuning when:** You want the model to learn a specific style, technical domain, or new behavior patterns.

## ğŸ“š Additional Documentation

For technical details about architecture and configuration, see [CLAUDE.md](CLAUDE.md).

## ğŸ“„ License

This project is open source and available for educational and experimental use.

## ğŸ¤ Contributions

Contributions are welcome. Feel free to open issues or pull requests.
